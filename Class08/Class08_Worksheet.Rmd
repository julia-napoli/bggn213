---
title: "Machine Learning 1"
author: "Julia Napoli"
date: "10/22/2021"
output:
  pdf_document: default
  html_document: default
---

# Clustering methods

Kmeans clustering in R is done with the 'kmeans()' function.

Here we make up some data to test and learn with.

```{r}
temp <- c(rnorm(30,3),rnorm(30,-3))
data <- cbind(x=temp,y=rev(temp))
plot(data)
hist(data)
```
Run 'kmeans()' set k to 2 nstart 20. The thing with Kmeans is you have to tell it how many clusters you want.

```{r}
km <- kmeans(data, centers = 2, nstart = 20)
km
```

> Q. How many points are in each cluster?

```{r}
km$size
```

> Q. What 'component' of your result object details cluster assignment/membership?

```{r}
km$cluster
```

> Q. What 'component' of your result object details cluster center?

```{r}
km$centers
```

> Q. Plot x colored by the kmeans cluster assignment and add cluster centers as blue points

```{r}
plot(data, col = km$cluster)
points(km$centers, col = "blue", pch = 15, cex=2)
```

# Heirarchical clustering

We will use the `hclust()` function on the same data as before and see how this method works.

```{r}
hc <- hclust(dist(data))
hc
```

hclust has a plot method

```{r}
plot(hc)
abline(h=7, col = "red")
```
To find our membership vector we need to "cut" the tree (dendrogram) and for this we use the `cutree()` function and tell it the height to cut at.

```{r}
cutree(hc,h=7)
```

We can also use `cutree()` and state the number of clusters we want...

```{r}
grps <- cutree(hc,k=2)
```

```{r}
plot(data, col = grps)
```

# Principal Component Analysis (PCA)

PCA is a super useful analysis method when you have lots of dimensions in your data...

## PCA of UK food data

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
```

> Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
nrow(x)
ncol(x)
dim(x)
head(x)
```

```{r}
rownames(x) <- x[,1]
x <- x[,-1]
ncol(x)
x
```

```{r}
x <- read.csv(url, row.names=1)
head(x)
dim(x)
```

```{r}
y <- as.matrix(x)
barplot(y, col = rainbow(nrow(y)), beside = TRUE)
```

```{r}
mycols <- rainbow(nrow(x))
pairs(x, col=mycols ,pch = 16)
```

## PCA to the rescue!

Here we will use the base R function for PCA, which is called `prcomp()`. This function wants the transpose of our data.

```{r}
# t(x)
pca <- prcomp(t(x))
summary(pca)
plot(pca)

```

We want to score plot (aka PCA plot). Basically of PC1 vs PC2.

```{r}
attributes(pca)
```

We are after the pca$x component for this plot...

```{r}
plot(pca$x[,1:2])
text(pca$x[,1:2], labels = colnames(x))
```

We can also examine the PCA "loadings", which tell us how much the original variables contribute to each new PC...

```{r}
barplot(pca$rotation[,1], las = 2)
```

## One more PCA for today

```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```

```{r}
head(rna.data)
colnames(rna.data)
ncol(rna.data)
```

Let's do some RNA seq...

```{r}
pca.rna <-prcomp(t(rna.data), scale = TRUE)
```

```{r}
summary(pca.rna)
plot(pca.rna)
```

```{r}
plot(pca.rna$x[,1], pca.rna$x[,2], xlab="PC1", ylab="PC2")
text(pca.rna$x[,1:2], labels = colnames(rna.data))
```

